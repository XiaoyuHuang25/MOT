# -*- coding: utf-8 -*-
from stonesoup.base import Property
from stonesoup.dataassociator.probability import JPDA as BaseJPDA
from stonesoup.types.detection import MissedDetection
from stonesoup.types.hypothesis import ProbabilityJointHypothesis, SingleProbabilityHypothesis
from stonesoup.types.numeric import Probability

from trackers.utils.dataassociator import MurtySolver
from trackers.utils.hypothesiser import MultipleHypothesis


class JPDAwithNBest(BaseJPDA):
    """
    JPDA data associator that uses Murty's algorithm to enumerate the
    N-best joint association hypotheses and then forms a standard JPDA
    mixture over those hypotheses.
    """

    top_n_hypotheses: int = Property(
        default=5,
        doc=(
            "The number of hypotheses to return from the JPDA association. "
            "If the total number of joint hypotheses is less than this, all "
            "hypotheses will be returned; otherwise only the top N by cost "
            "are returned."
        ),
    )

    def enumerate_JPDA_hypotheses(self, tracks, multihypths):
        """
        Enumerate N-best joint association hypotheses using Murty's algorithm.

        Parameters
        ----------
        tracks : Iterable[Track]
            Current set of tracks.
        multihypths : dict[Track, MultipleHypothesis]
            Per-track set of single-target hypotheses, typically
            generated by `self.generate_hypotheses`.

        Returns
        -------
        list[ProbabilityJointHypothesis]
            List of joint hypotheses with normalized probabilities.
        """
        joint_hypotheses = []

        if not tracks:
            return joint_hypotheses

        track_list = []
        hypotheses_list = []
        for track, hypotheses in multihypths.items():
            track_list.append(track)
            hypotheses_list.append(hypotheses)

        # MurtySolver returns:
        #   cost_list: list of global costs
        #   global_hypotheses_indices: list[list[int]], one per joint hypothesis
        cost_list, global_hypotheses_indices = MurtySolver.murty_associate(
            hypotheses_list, self.top_n_hypotheses
        )

        for cost, hypotheses_indices in zip(cost_list, global_hypotheses_indices):
            local_hypotheses = {}
            for track_idx, hypothesis_idx in enumerate(hypotheses_indices):
                track = track_list[track_idx]
                hypothesis = multihypths[track][hypothesis_idx]
                local_hypotheses[track] = hypothesis
            joint_hypotheses.append(ProbabilityJointHypothesis(local_hypotheses))

        # Normalize probabilities across joint hypotheses
        if not joint_hypotheses:
            return joint_hypotheses

        sum_probabilities = Probability.sum(
            hypothesis.probability for hypothesis in joint_hypotheses
        )

        for hypothesis in joint_hypotheses:
            hypothesis.probability = Probability(
                hypothesis.probability.log_value - sum_probabilities.log_value,
                log_value=True,
            )

        return joint_hypotheses

    def associate(self, tracks, detections, timestamp, **kwargs):
        """
        Perform JPDA association using Murty N-best joint hypotheses.

        Parameters
        ----------
        tracks : set[Track]
            Set of existing tracks.
        detections : set[Detection]
            Set of detections at the current timestamp.
        timestamp : datetime-like
            Current time.

        Returns
        -------
        dict[Track, MultipleHypothesis]
            For each track, a MultipleHypothesis over the measurement
            hypotheses (including missed detections) with JPDA probabilities.
        """
        # 1) Build per-track MultipleHypothesis objects from detections
        hypotheses = self.generate_hypotheses(
            tracks, detections, timestamp, **kwargs
        )

        # 2) Enumerate joint hypotheses (Murty + normalization)
        joint_hypotheses = self.enumerate_JPDA_hypotheses(tracks, hypotheses)

        new_hypotheses = {}

        for track in tracks:
            single_measurement_hypotheses = []

            # ---- (a) Missed detection hypothesis for this track ----
            prob_misdetect_list = [
                joint_hypothesis.probability
                for joint_hypothesis in joint_hypotheses
                if not joint_hypothesis.hypotheses[track].measurement
            ]
            if prob_misdetect_list:
                prob_misdetect = Probability.sum(prob_misdetect_list)
                single_measurement_hypotheses.append(
                    SingleProbabilityHypothesis(
                        hypotheses[track][0].prediction,
                        MissedDetection(timestamp=timestamp),
                        measurement_prediction=hypotheses[track][0].measurement_prediction,
                        probability=prob_misdetect,
                    )
                )

            # ---- (b) Hypotheses for each measurement associated to this track ----
            for hypothesis in hypotheses[track]:
                if not hypothesis:
                    continue

                pro_detect_assoc = Probability.sum(
                    joint_hypothesis.probability
                    for joint_hypothesis in joint_hypotheses
                    if (
                        joint_hypothesis.hypotheses[track].measurement
                        is hypothesis.measurement
                    )
                )

                single_measurement_hypotheses.append(
                    SingleProbabilityHypothesis(
                        hypothesis.prediction,
                        hypothesis.measurement,
                        measurement_prediction=hypothesis.measurement_prediction,
                        probability=pro_detect_assoc,
                    )
                )

            # Wrap in a MultipleHypothesis for StoneSoup compatibility
            result = MultipleHypothesis(single_measurement_hypotheses, True, 1)
            new_hypotheses[track] = result

        return new_hypotheses
